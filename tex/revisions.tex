% !TEX encoding = UTF-8 Unicode

% Compilation using 'acmsmall.cls' - version 1.3 (March 2012), Aptara Inc.
% (c) 2010 Association for Computing Machinery (ACM)
%
% Questions/Suggestions/Feedback should be addressed to => "acmtexsupport@aptaracorp.com".
% Users can also go through the FAQs available on the journal's submission webpage.
%
% Steps to compile: latex, bibtex, latex, latex


\documentclass[prodmode,acmtochi]{acmsmall} % Aptara syntax

% Package to generate and customize Algorithm as per ACM style
\usepackage[ruled]{algorithm2e}
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

% Packages
\usepackage[super]{nth}
\usepackage[inline]{enumitem}
\usepackage{moreenum}
\usepackage{tabu}
\usepackage{booktabs}
\usepackage{array}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}

% double rule
\newcommand{\doublerule}[1][.4pt]{%
  \noindent
  \makebox[0pt][l]{\rule[.7ex]{\linewidth}{#1}}%
  \rule[.3ex]{\linewidth}{#1}}

\setlength\parindent{0pt}

% Metadata Information
\acmVolume{0}
\acmNumber{0}
\acmArticle{0}
\acmYear{0}
\acmMonth{0}

% Copyright
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

% DOI
\doi{0000001.0000001}

%ISSN
\issn{1234-56789}

% Document starts
\begin{document}

% Page heads
\markboth{B. Harmon et al.}{Cognitively Grasping Topography with Tangible Landscape}

% Title portion
\title{Cognitively Grasping Topography with Tangible Landscape}
\author{BRENDAN ALEXANDER HARMON
\affil{North Carolina State University}
ANNA PETRASOVA
\affil{North Carolina State University}
VACLAV PETRAS
\affil{North Carolina State University}
HELENA MITASOVA
\affil{North Carolina State University}
ROSS ​KENDALL MEENTEMEYER
\affil{North Carolina State University}
EUGENE BRESSLER
\affil{North Carolina State University}
ART RICE
\affil{North Carolina State University}}

\maketitle

\textbf{Revisions}

\vspace*{3em}

%Dear editors and reviewers,\\
%
%We have revised the paper 
%\emph{Cognitively Grasping Topography with Tangible Landscape}. 
%
%\hrulefill \\

\textbf{Editor:} Provide more details in related work section 
that differentiates your findings from related work.\\

\textbf{Reviewer 1:} For a journal article, a more detailed discussion would be appropriate... What were the results/conclusions of such prior evaluations?\\

\hrulefill \\

We have added a detailed literature review to subsection 
\textbf{1.2~Tangible interfaces for geospatial modeling}
that details relevant TUIs and user studies.
We discuss TUI including 
Project FEELEX, 
the XenoVision Mark III Dynamic Sand Table,  
the Northrop Grumman Terrain Table, 
Relief, Recompose, inFORM, Tangible CityScape,
Urp, the Collaborative Design Platform, 
Illuminating Clay, Tangible Geospatial Modeling System, 
SandScape, Phoxel-Space, 
the Augmented Reality Sandbox,
Hakoniwa, the Augmented REality Sandtable,
Inner Garden, and Tangible Landscape. 
The review of previous systems also includes 
conclusions from prior studies and  evaluations.
We describe what is different about Tangible Landscape 
at the end of subsection \textbf{2.1~Concept}.\\

\doublerule\\

\textbf{Editor:} Add details on the system implementation... Please add physical dimensions, capturing distances, lag, what contributes to 2 sec. delay?, etc. \\

\textbf{Reviewer 1:} From a technical point of view, the presented system seems to be a solid piece of work. Yet, the authors do not describe it in detail\ldots The feedback rate how has (only) two seconds of lag. Is this fast enough for users to work interactively?\\

\textbf{Reviewer 2:} What is the physical size of the sand interface?
At the mounted distance of the Kinect, what is the scanning resolution in x,y, and z, and how much noise does the sensor produce? What is the sampling rate? Does the system filter and average the sensor data over time or drop frames and only use raw data?
What is the system lag? \ldots System lag seems like a critical factor for the projected feedback in the difference experiment and the water flow experiment. What are the observations of the authors in this regard?\\

\hrulefill \\

In subsection \textbf{2.3~Implementation} 
we explain how the process of calibration, scanning, filtering,
terrain modeling, and analysis works.\\

In subsection \textbf{2.4~System resolution, accuracy, and speed} 
we describe Tangible Landscape's capturing distance and lag,
describe the range of physical model sizes,
quantify its resolution and
assess its accuracy and speed.
The accuracy assessment and benchmarks are presented 
in Fig.~7, Table~VI, and Table~VII. \\

We discuss the effect of lag on interaction 
briefly in \textbf{2.2~Design} and in more detail in
\textbf{6.3~Reflections on the design process}.\\

We added a diagram of the system setup with measurements to 
\textbf{Appendix A}. \\

We briefly outline potential applications in 
subsection  \textbf{2.6~Applications}. \\

\doublerule\\

\textbf{Editor:} Clarify user study details. \\

\textbf{Reviewer 1:} The user studies leave many questions open, regarding the test design and the demographics of the recruited test persons. It sounds like all participants performed the three modeling tasks in the same order \ldots working with the same desired 3D model. Were there any learning effects? How experienced were the test persons with modeling tasks and/or geographical datasets / maps? \ldots Apparently, only some, selected test persons were interviewed. Which criterion was used for deciding whom to interview and whom not?\\

\textbf{Reviewer 2:}
Study Participants:
As this study investigates expert users, have the participants used CAD programs before? If so, how frequently do they use it in their work? Are they familiar and comfortable with Rhinoceros, or do they use other CAD software? How many of the participants have worked with GIS models before and have familiarity with contour lines and elevation models? This context is interesting as it relates back to whether the study was aimed at beginners or expert users.\\

Study Design:
As the authors use a within-subjects design, do they take any measures to minimize the practice effect, such as a counterbalance design?
What version of Rhinoceros is used for the study? \ldots Did study participants for instance produce more precise results with Vue compared to Rhinoceros?
The system mentions two tools provided for the physical modeling tasks: a 3D scale and a wooden sculpting tool.  Where the subjects advised to use these tools? 
Was this model random or did subjects have access to a physical reference model in this condition as well?
Why did the study switch to another reference model for the Difference Experiment?
How was the time constraint of 10 minutes chosen? \\

\clearpage

\hrulefill \\

In subsection \textbf{3.1~Methods}
in the paragraph \textbf{Participants} and Table VIII
we describe the participants 
and their experience with spatial thinking, GIS, and 3D modeling. \\

In the paragraph \textbf{Experimental design}
we describe the methodology for the Coupling experiment
in more detail including tool use, the models, 
the time limit, counterbalancing, and interviews.
We also discuss the time limit in more detail 
in the paragraph \textbf{Digital modeling}.

In the paragraph \textbf{Digital modeling}
we discuss in detail the choice of 3D modeling software
comparing the pros and cons of different programs. \\

Observed tool use is discussed in subsection \textbf{3.2~Results} and in the discussion in subsection \textbf{6.1~Coupling physical and digital models}.\\

The rationale for using different models 
for the difference and water flow experiments
is implicit in the opening paragraph of 
section \textbf{4~Difference experiment} -- 
the aim of these experiments was not to compare, 
but to study process. \\

\doublerule\\

\textbf{Editor:} How proficient in Rhino were your participants? \\

\textbf{Reviewer 1:} How experienced were the test persons with modeling tasks and/or geographical datasets / maps? \\

\textbf{Reviewer 2:} I believe that TOCHI readers will be interested in the implications whether such TUI modeling systems can be suitable not only for novice users, but also for use by experts, and what functionality can support such modeling tasks. \\

\hrulefill \\

We reframed the study to compare beginners and experts.
After describing the participants in 
subsection \textbf{3.1~Methods}
in the paragraph \textbf{Participants},
we recomputed the analyses
to in order compare novices versus experts.
We used pairwise comparison to compare their performance 
(See Fig.~17). \\

Subsection \textbf{3.2~Results} presents the new results 
with new Tables X-XVI comparing novices versus experts. 
In these tables we changed the color table for standard deviation
and cited its source -- Color Brewer -- and references in publication. \\

Table XXIII in Subsection \textbf{4.2~Results}
presents the new results comparing novices versus experts
for the difference experiment. \\

Table XXVII in Subsection \textbf{5.2~Results}
presents the new results comparing novices versus experts
for the water flow experiment.  \\

\doublerule \\

\textbf{Reviewer 2:}
Study results:
In the results of the coupling experiment reported on page 15, the authors describe the Rhinoceros models as approximate and abstract. I assume that is the case either because the users did not create enough NURBS control points, and/or because the control points were not accurate in how they corresponded to the reference model. Can the authors provide more details on what factor seemed more influential? For instance, did users mainly work with the 10x10 grid of initial control points or did they also make use of the feature to rebuild the surface to a higher density of control points?
I was also interested in the choice of color scheme for the heat maps, some of which I also found harder to read than others. An example are the colors for the stdev. of difference in Table VI\ldots
It would be very interesting to find out more about the qualitative feedback from users, as outlined in the interview guidelines on page 34. Additionally, the authors mention direct observation, photographic and video analysis in the methods for the coupling experiment on page 12, but do not report many observations from these, such as usage patterns or other observations in their results. These observations, like the use of tools in different conditions, would be very helpful for TEI researchers. \\

\hrulefill \\
We discuss rebuilding of control points in 
subsection \textbf{3.2~Results} 
and in the discussion in subsection 
\textbf{6.1~Coupling physical and digital models}.
highlighting the difference between novices and experts.
Then we discuss the implications. 

We refined many of the color tables. 
As already noted we changed the color table for standard deviation
in all tables and cited its source, Color Brewer.\\

More feedback from interviews and observations are discussed in the subsections \textbf{3.2~Results}, \textbf{4.2~Results}, and \textbf{5.2~Results}.
Table XXVIII compiles select comments from interviews. \\

\doublerule \\

\textbf{Editor:} Generalize on your findings.\\

\hrulefill \\

In section \textbf{6~Discussion}
we discuss the new results 
comparing novices' and experts' performance and process, 
draw generalized conclusions, and 
hypothesize about the implications. \\

\doublerule \\

\textbf{Reviewer 1:} The authors formulate a number of compelling questions - yet, most of them could not (yet) be addressed in the submitted paper.\\

\hrulefill \\

To clearly address the research questions, the discussion
is broken in discrete sections -- 
subsection \textbf{6.1~Coupling physical and digital models}
and 
subsection \textbf{6.2~How tangible geospatial analytics mediate users’ 3D spatial performance} 
-- addressing the questions. \\

The revised results and discussion are reflected in section \textbf{8~Conclusion}.\\

\doublerule \\

\textbf{Reviewer 2:} Longitudinal results: As the functionality outlined in the study experiments has been part of the Tangible Landscape system for many years, the paper should balance the results of this short study with qualitative observations of experts that have been using it over a longer duration, and how their use patterns shift.\\

\hrulefill \\

The authors' experiences and observations are discussed in 
subsection \textbf{6.3~Reflections on the design process}.
We discuss system lag / speed, digitizing hands and arms,
and unstructured versus structured users experiences. 
This subsection leads to our proposed design guidelines 
in the next subsection.\\

\doublerule \\

\textbf{Editor:} Best practices if spatial modeling is the target goal.\\

\hrulefill \\

Subsection \textbf{6.4~Design guidelines} outlines best practices
for design TUIs for spatial modeling. \\

\doublerule \\

\textbf{Suggestions for Online Appendix Content} \\

\textbf{Reviewer 2:} 
Videos: 
The videos should definitely include a detailed video that shows all five experiments for the study.\\

Data Sets:
In addition, it would be great if the authors can also provide the data sets resulting from their user study for comparison and further analysis. Ideally, this would include the reference models and the raw and processed elevation data of the models that the users created in the five experiments.\\

\hrulefill \\

As supplemental content
we added videos demonstrating training, 
demonstrating each of the experiments 
and showcasing applications with Tangible Landscape.\\

We have also added code and data for running the experiment
as supplemental content. The data includes anonymized, 
but otherwise raw map data as well as the analyzed results.
The results are reproducible using the code and data.\\


\bibliographystyle{ACM-Reference-Format-Journals}
\bibliography{tangible_topography.bib}

\end{document}

