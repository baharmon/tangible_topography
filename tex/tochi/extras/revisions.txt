
Editor: Provide more details in related work section that differentiates your findings from related work.

Reviewer 1: For a journal article, a more detailed discussion would be appropriate... What were the results/conclusions of such prior evaluations?

-------------------------------------------------------------------------------

We have added a detailed literature review to subsection 1.2 Tangible interfaces for geospatial modeling that details relevant TUIs and user studies. We discuss TUI including Project FEELEX, the XenoVision Mark III Dynamic Sand Table, the Northrop Grumman Terrain Table, Relief, Recompose, inFORM, Tangible CityScape, Urp, the Collaborative Design Platform, Illuminating Clay, Tangible Geospatial Modeling System, SandScape, Phoxel-Space, the Augmented Reality Sandbox, Hakoniwa, the Augmented REality Sandtable, Inner Garden, and Tangible Landscape. The review of previous systems also includes conclusions from prior studies and evaluations. We describe what is different about Tangible Landscape at the end of subsection 2.1 Concept.

===============================================================================

Editor: Add details on the system implementation... Please add physical dimensions, capturing distances, lag, what contributes to 2 sec. delay?, etc.

Reviewer 1: From a technical point of view, the presented system seems to be a solid piece of work. Yet, the authors do not describe it in detail...The feedback rate how has (only) two seconds of lag. Is this fast enough for users to work interactively?

Reviewer 2: What is the physical size of the sand interface? At the mounted distance of the Kinect, what is the scanning resolution in x, y, and z, and how much noise does the sensor produce? What is the sampling rate? Does the system filter and average the sensor data over time or drop frames and only use raw data? What is the system lag? . . . System lag seems like a critical factor for the projected feedback in the difference experiment and the water flow experiment. What are the observations of the authors in this regard?

-------------------------------------------------------------------------------

In subsection 2.3 Implementation we explain how the process of calibration, scanning, filtering, terrain modeling, and analysis works.
In subsection 2.4 System resolution, accuracy, and speed we describe Tangible Landscape’s capturing distance and lag, describe the range of physical model sizes, quantify its resolution and assess its accuracy and speed. The accuracy assessment and benchmarks are presented in Fig. 7, Table VI, and Table VII.
We discuss the effect of lag on interaction briefly in 2.2 Design and in more detail in 6.3 Reflections on the design process.
We added a diagram of the system setup with measurements to Appendix A. We briefly outline potential applications in subsection 2.6 Applications.

===============================================================================

Editor: Clarify user study details.

Reviewer 1: The user studies leave many questions open, regarding the test design and the demographics of the recruited test persons. It sounds like all participants performed the three modeling tasks in the same order ...working with the same desired 3D model. Were there any learning effects? How experienced were the test persons with modeling tasks and/or geographical datasets / maps? . . . Apparently, only some, selected test persons were interviewed. Which criterion was used for deciding whom to interview and whom not?

Reviewer 2: Study Participants: As this study investigates expert users, have the participants used CAD programs before? If so, how frequently do they use it in their work? Are they familiar and comfortable with Rhinoceros, or do they use other CAD software? How many of the participants have worked with GIS models before and have familiarity with contour lines and elevation models? This context is interesting as it relates back to whether the study was aimed at beginners or expert users.
Study Design: As the authors use a within-subjects design, do they take any measures to minimize the practice effect, such as a counterbalance design? What version of Rhinoceros is used for the study? . . . Did study participants for instance produce more precise results with Vue compared to Rhinoceros? The system mentions two tools provided for the physical modeling tasks: a 3D scale and a wooden sculpting tool. Where the subjects advised to use these tools? Was this model random or did subjects have access to a physical reference model in this condition as well? Why did the study switch to another reference model for the Difference Experiment? How was the time constraint of 10 minutes chosen?

-------------------------------------------------------------------------------

In subsection 3.1 Methods in the paragraph Participants and Table VIII we describe the participants and their experience with spatial thinking, GIS, and 3D modeling.
In the paragraph Experimental design we describe the methodology for the Coupling experiment in more detail including tool use, the models, the time limit, counter- balancing, and interviews. We also discuss the time limit in more detail in the paragraph Digital modeling.
In the paragraph Digital modeling we discuss in detail the choice of 3D modeling software comparing the pros and cons of different programs.
Observed tool use is discussed in subsection 3.2 Results and in the discussion in subsection 6.1 Coupling physical and digital models.
The rationale for using different models for the difference and water flow experiments is implicit in the opening paragraph of section 4 Difference experiment – the aim of these experiments was not to compare, but to study process.

===============================================================================

Editor: How proficient in Rhino were your participants?

Reviewer 1: How experienced were the test persons with modeling tasks and/or
geographical datasets / maps?

Reviewer 2: I believe that TOCHI readers will be interested in the implications whether such TUI modeling systems can be suitable not only for novice users, but also for use by experts, and what functionality can support such modeling tasks.

-------------------------------------------------------------------------------

We reframed the study to compare beginners and experts. After describing the participants in subsection 3.1 Methods in the paragraph Participants, we recomputed the analyses to in order compare novices versus experts. We used pairwise comparison to compare their performance (See Fig. 17).
Subsection 3.2 Results presents the new results with new Tables X-XVI comparing novices versus experts. In these tables we changed the color table for standard deviation and cited its source – Color Brewer – and references in publication.
Table XXIII in Subsection 4.2 Results presents the new results comparing novices versus experts for the difference experiment.
Table XXVII in Subsection 5.2 Results presents the new results comparing novices versus experts for the water flow experiment.

===============================================================================

Reviewer 2: Study results: In the results of the coupling experiment reported on page 15, the authors describe the Rhinoceros models as approximate and abstract. I assume that is the case either because the users did not create enough NURBS control points, and/or because the control points were not accurate in how they corresponded to the reference model. Can the authors provide more details on what factor seemed more influential? For instance, did users mainly work with the 10x10 grid of initial control points or did they also make use of the feature to rebuild the surface to a higher density of control points? I was also interested in the choice of color scheme for the heat maps, some of which I also found harder to read than others. An example are the colors for the stdev. of difference in Table VI. . . It would be very interesting to find out more about the qualitative feedback from users, as outlined in the interview guidelines on page 34. Additionally, the authors mention direct observation, photographic and video analysis in the methods for the coupling experiment on page 12, but do not report many observations from these, such as usage patterns or other observations in their results. These observations, like the use of tools in different conditions, would be very helpful for TEI researchers.

-------------------------------------------------------------------------------

We discuss rebuilding of control points in subsection 3.2 Results and in the discussion in subsection 6.1 Coupling physical and digital models. highlighting the difference between novices and experts. Then we discuss the implications.
We refined many of the color tables. As already noted we changed the color table for standard deviation in all tables and cited its source, Color Brewer.
More feedback from interviews and observations are discussed in the subsections 3.2 Results, 4.2 Results, and 5.2 Results. Table XXVIII compiles select comments from interviews.

===============================================================================

Editor: Generalize on your findings.

-------------------------------------------------------------------------------

In section 6 Discussion we discuss the new results comparing novices’ and experts’ performance and process, draw generalized conclusions, and hypothesize about the implications.

===============================================================================

Reviewer 1: The authors formulate a number of compelling questions - yet, most of them could not (yet) be addressed in the submitted paper.

-------------------------------------------------------------------------------

To clearly address the research questions, the discussion is broken in discrete sections – subsection 6.1 Coupling physical and digital models and subsection 6.2 How tangible geospatial analytics mediate users 3D spatial performance – addressing the questions.
The revised results and discussion are reflected in section 8 Conclusion.

===============================================================================

Reviewer 2: Longitudinal results: As the functionality outlined in the study experiments has been part of the Tangible Landscape system for many years, the paper should balance the results of this short study with qualitative observations of experts that have been using it over a longer duration, and how their use patterns shift.

-------------------------------------------------------------------------------

The authors’ experiences and observations are discussed in subsection 6.3 Reflections on the design process. We discuss system lag / speed, digitizing hands and arms, and unstructured versus structured users experiences. This subsection leads to our proposed design guidelines in the next subsection.

===============================================================================

Editor: Best practices if spatial modeling is the target goal.

-------------------------------------------------------------------------------

Subsection 6.4 Design guidelines outlines best practices for design TUIs for spatial modeling.

===============================================================================

Suggestions for Online Appendix Content
Reviewer 2: Videos: The videos should definitely include a detailed video that shows all five experiments for the study.
Data Sets: In addition, it would be great if the authors can also provide the data sets resulting from their user study for comparison and further analysis. Ideally, this would include the reference models and the raw and processed elevation data of the models that the users created in the five experiments.

-------------------------------------------------------------------------------

As supplemental content we added videos demonstrating training, demonstrating each of the experiments and showcasing applications with Tangible Landscape.
We have also added code and data for running the experiment as supplemental content. The data includes anonymized, but otherwise raw map data as well as the analyzed results. The results are reproducible using the code and data.
